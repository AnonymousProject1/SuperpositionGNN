import numpy as np

# ---- Hidden embeddings as a dict of 12 vectors of length 6 ----
embeddings_dict = {
    "(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0)": [2.893054723739624, 0.38600125908851624, -0.15847396850585938, -1.2159414291381836, -0.6308308839797974, 0.441690057516098],
    "(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0)": [1.7787766456604004, -1.2702299356460571, 0.04499184712767601, -2.5943713188171387, -1.304353952407837, -1.42259681224823],
    "(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)": [0.38495633006095886, -0.44728243350982666, 1.5486596822738647, -1.9879368543624878, -1.485970139503479, 0.8248305916786194],
    "(0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)": [-0.785186231136322, -1.7758259773254395, 0.9602509140968323, 0.4585796296596527, -3.0179128646850586, 1.6529511213302612],
    "(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)": [1.007570743560791, -0.7236474752426147, -0.11067365109920502, -0.8377350568771362, -1.7549583911895752, 0.478305846452713],
    "(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)": [1.009816288948059, -0.7294455766677856, -0.1029336005449295, -0.8385892510414124, -1.75617516040802, 0.4757305979728699],
    "(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0)": [-0.9920541644096375, -1.124616026878357, -1.4501068592071533, -1.8595855236053467, -2.3244869709014893, -0.5100272297859192],
    "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)": [0.993495762348175, -0.7257117629051208, -0.10905259102582932, -0.8270336389541626, -1.7626926898956299, 0.48985567688941956],
    "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0)": [0.5824981927871704, 0.02782451920211315, -1.1123062372207642, -0.6128752827644348, -1.6663495302200317, 0.940508246421814],
    "(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)": [1.7721027135849, -0.9646974802017212, -2.315410614013672, 0.2635424733161926, -1.973621129989624, -0.1678830087184906],
    "(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)": [2.735187530517578, -0.82239830493927, 1.4110513925552368, 0.03588707372546196, -1.4310835599899292, 1.130239725112915],
    "(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)": [0.9969518780708313, -0.723141074180603, -0.10885954648256302, -0.8232346177101135, -1.7615143060684204, 0.4935944974422455]
}

# Convert to array of shape (12, 6)
embed_array = np.array(list(embeddings_dict.values()))  # each row is one embedding

# ---- The final linear out weights: shape (12, 6) ----
linear_out_weights = np.array([
    [-2.1382250785827637,  2.734501361846924,  2.4741592407226562, -1.9632494449615479,  1.1663308143615723,  0.6307355165481567],
    [ 1.9076085090637207, -1.9574525356292725,  2.4156501293182373,  2.9956297874450684,  1.5524203777313232, -1.1938825845718384],
    [-0.24008092284202576, 0.04398638755083084, 0.09979632496833801, 0.21870525181293488, 0.5163353085517883, -0.3261415958404541],
    [-3.9809775352478027, -3.933863639831543,  2.906670331954956,   4.34714937210083,   -0.5311094522476196,  1.0321670770645142],
    [-0.17747031152248383, -0.045450061559677124, 0.23657113313674927, 0.3247137665748596,  0.26056957244873047, -0.10201159119606018],
    [ 0.9727454781532288,  -3.418027877807617,   2.213045120239258,  -0.6742205023765564, 2.4193203449249268,  -6.817205429077148],
    [ 2.13211727142334,    -2.480015754699707,  -4.625438690185547,   6.142148971557617,   0.49114206433296204, -3.4690210819244385],
    [-0.30989474058151245, -0.00609633419662714, 0.1228891983628273,  0.3034780025482178,  0.37316542863845825, -0.2512141764163971],
    [-3.989217519760132,   -0.021693656221032143, -1.54346764087677,   -1.1246799230575562,  0.48579972982406616, -1.8082045316696167],
    [ 1.4795390367507935,   4.158040523529053,   -0.32315537333488464, -0.3055075407028198,  2.1859071254730225,  -0.15133315324783325],
    [-1.9512720108032227,   5.577139854431152,   -1.8162469863891602,   0.6230971813201904,  0.12373887747526169,  3.2777600288391113],
    [-0.2640875577926636,   0.045823901891708374, 0.0894245058298111,   0.2726385295391083,  0.37953731417655945, -0.1551440805196762]
])

# shape: (12, 12) => each row i is the dot products of embedding i with all weight rows
all_dot_products = embed_array.dot(linear_out_weights.T)

# all_dot_products[i] is a length-12 vector of what the linear layer would produce
# for embedding i, ignoring any bias.
print(all_dot_products)